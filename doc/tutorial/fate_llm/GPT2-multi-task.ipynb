{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Task Federated Learning with GPT-2 using FATE-LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will explore the implementation of multi-task federated learning with LM: GPT-2 using the FATE-LLM framework. FATE-LLM provides  the \"pellm\" module for efficient federated learning. It is specifically designed for large language models in a federated setting.\n",
    "\n",
    "Multi-task learning involves training a model to perform multiple tasks simultaneously. In this tutorial, we will focus on two tasks - sentiment classification and named entity recognition (NER) - and show how they can be combined with GPT-2 in a federated learning setting. We will use the IMDB sentiment analysis dataset and the CoNLL-2003 NER dataset for our tasks.\n",
    "\n",
    "Additionally, by leveraging the Adapter mechanism, we can effectively reduce communication volume and improve overall efficiency in our federated learning setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2\n",
    "\n",
    "GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. GPT-2 is trained with a causal language modeling (CLM) objective, conditioning on a left-to-right context window of 1024 tokens. In this tutorial, we will use GPT2, you can download the pretrained model from [here](https://huggingface.co/gpt2) (We choose the smallest version for this tutorial), or let the program automatically download it when you use it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: IMDB Sentimental and CoNLL-2003 NER\n",
    "\n",
    "About IMDB Sentimental Dataset:\n",
    "\n",
    "The IMDB dataset is a binary classification dataset containing movie reviews with positive or negative sentiment labels. We will use this dataset as one of our tasks in the multi-task learning setup. You can download our processed dataset from here:\n",
    "\n",
    "The original data is from:\n",
    "\n",
    "https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "About CoNLL-2003 NER Dataset:\n",
    "\n",
    "The CoNLL-2003 NER dataset is a widely used benchmark dataset for named entity recognition tasks. It contains English and German news articles with named entity annotations for person, organization, and location names. We will use this dataset as another task in our multi-task learning setup. The official website:\n",
    "\n",
    "https://www.clips.uantwerpen.be/conll2003/ner/\n",
    "We will use the Hugging Face transformers library to preprocess the text data and tokenize it for use in our multi-task federated learning task. The processed data can be found here:\n",
    "\n",
    "In this tutorial we will use datasets module provided by huggingface to download these two datasets. We then save the dataset instance to the file system for later use.\n",
    "\n",
    "## Download and Cache Dataset\n",
    "\n",
    "In this example, for the ease of display, we will use cache the training set only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datasets import load_dataset\n",
    "\n",
    "imdb_dataset = load_dataset(\"imdb\", download_mode=\"reuse_cache_if_exists\")\n",
    "train_dataset_imdb = imdb_dataset[\"train\"]\n",
    "val_dataset_imdb = imdb_dataset[\"test\"]\n",
    "\n",
    "conll_dataset = load_dataset(\"conll2003\", download_mode=\"reuse_cache_if_exists\")\n",
    "train_dataset_conll = conll_dataset[\"train\"]\n",
    "\n",
    "# Save the dataset\n",
    "pickle.dump(train_dataset_imdb, open('./train_dataset_imdb.pkl', 'wb'))\n",
    "pickle.dump(train_dataset_conll, open('./train_dataset_conll.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_ds = pickle.load(open('./train_dataset_imdb.pkl', 'rb'))\n",
    "conll_ds = pickle.load(open('./train_dataset_conll.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we realiaze a MultiTaskDataset class based on FATE-datset class to mix these two dataset together. For more details of FATE dataset setting, we recommend that you read through these tutorials first: [NN Dataset Customization](./Homo-NN-Customize-your-Dataset.ipynb).\n",
    "Our dataset directly load the cached pickle dataset from a path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.component.nn import save_to_fate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%save_to_fate dataset multitask_ds.py\n",
    "import pickle\n",
    "import torch as t\n",
    "import tqdm\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import os\n",
    "from federatedml.nn.dataset.base import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# avoid tokenizer parallelism\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "sentiment_task_prefix = \"sentiment: \"\n",
    "ner_task_prefix = \"ner: \"\n",
    "\n",
    "\n",
    "def add_sentiment_task_prefix(example):\n",
    "    example[\"text\"] = sentiment_task_prefix + example[\"text\"]\n",
    "    return example\n",
    "\n",
    "\n",
    "\n",
    "def add_ner_task_prefix(example):\n",
    "    example[\"tokens\"] = [ner_task_prefix] + example[\"tokens\"]\n",
    "    return example\n",
    "\n",
    "\n",
    "\n",
    "class MultiTaskDataset(Dataset):\n",
    "    \"\"\"MultiTaskDataset\n",
    "    Args:\n",
    "        take_limits: take how many samples from each dataset\n",
    "        shuffle_seed: shuffle seed\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer_name_or_path='gpt2', take_limits=None, shuffle_seed=114514):\n",
    "        self.ds = []\n",
    "        self.labels = []\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.add_prefix_space = True\n",
    "        self.ner_tag_num = 0\n",
    "        self.shuffle_seed = shuffle_seed\n",
    "        self.take_limits = take_limits\n",
    "\n",
    "    def convert_imdb_example(self, example):\n",
    "        encodings = self.tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "        return {\"input_ids\": encodings[\"input_ids\"], \"attention_mask\": encodings[\"attention_mask\"], \"labels\": example[\"label\"], \"task_type\": 0}\n",
    "\n",
    "    def convert_conll_example(self, example):\n",
    "        encodings = self.tokenizer(example[\"tokens\"], is_split_into_words=True, truncation=True, padding=\"max_length\", max_length=128)\n",
    "        labels = [-100] * (128 - len(example[\"ner_tags\"]) - 1) + [-100] + example[\"ner_tags\"]\n",
    "        return {\"input_ids\": encodings[\"input_ids\"], \"attention_mask\": encodings[\"attention_mask\"], \"labels\": labels, \"task_type\": 1}\n",
    "\n",
    "    def load(self, path):\n",
    "        imdb_ds = pickle.load(open(path + '/train_dataset_imdb.pkl', 'rb'))\n",
    "        conll_ds = pickle.load(open(path + '/train_dataset_conll.pkl', 'rb'))\n",
    "        imdb_ds= imdb_ds.map(add_sentiment_task_prefix)\n",
    "        conll_ds = conll_ds.map(add_ner_task_prefix)\n",
    "        imdb_ds = imdb_ds.map(self.convert_imdb_example)\n",
    "        conll_ds = conll_ds.map(self.convert_conll_example)\n",
    "        self.ner_tag_num = conll_ds.features['ner_tags'].feature.num_classes\n",
    "\n",
    "        if self.take_limits is not None:\n",
    "            imdb = imdb_ds.shuffle(seed=self.shuffle_seed).select(range(self.take_limits))\n",
    "            conll = conll_ds.shuffle(seed=self.shuffle_seed).select(range(self.take_limits))\n",
    "        else:\n",
    "            imdb = imdb_ds\n",
    "            conll = conll_ds\n",
    "        for i in tqdm.tqdm(range(len(imdb))):\n",
    "            self.ds.append({'input_ids': t.LongTensor(imdb[i]['input_ids']), 'attention_mask': t.LongTensor(imdb[i]['attention_mask']), 'task_type': t.LongTensor([0])})\n",
    "            self.labels.append(t.LongTensor([imdb[i]['labels']]))\n",
    "        \n",
    "        for i in tqdm.tqdm(range(len(conll))):\n",
    "            self.ds.append({'input_ids': t.LongTensor(conll[i]['input_ids']), 'attention_mask': t.LongTensor(conll[i]['attention_mask']), 'task_type': t.LongTensor([1])})\n",
    "            self.labels.append(t.LongTensor(conll[i]['labels']))\n",
    "\n",
    "        # padding the binary classification labels to match the length of the ner labels\n",
    "        self.labels = rnn_utils.pad_sequence(self.labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.ds[index], self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MultiTaskDataset(tokenizer_name_or_path='your path', take_limits=500)\n",
    "ds.load('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([34086,  3681,    25,  2293,   262, 21840,   317, 15154,    39,  1677,\n",
       "              7, 16942,     8,   543,   373,   257, 28763,   286,   257, 28600,\n",
       "           7246,   711,   339,  2058,   351, 16400,    48,    51,   543,  1165,\n",
       "           3073,   588,   257,  3800,   711,    27,  1671,  1220,  6927,  1671,\n",
       "          11037,   818,  3800,  5341,    11,   356,   423,  3435, 19642,    11,\n",
       "            625, 27362,   994,  1165,   262,   976,    27,  1671,  1220,  6927,\n",
       "           1671, 11037,   464,   717,  2063,  2523, 48148,   397,    71,  2048,\n",
       "          26471,   262,  2319,    10,   317, 50133,   323, 26105,   508,  6529,\n",
       "           1165,  8258,   588,   257,  1402, 34712,    27,  1671,  1220,  6927,\n",
       "           1671, 11037,   464,  2646,   468,   257,   922,  3275,   703,   407,\n",
       "            284, 20851,   534,  3367,   475, 21098,   262,   835, 48148,   397,\n",
       "             71,  3382,   284,   787,  9084,  4106,  4497,   318,  5543,  8390,\n",
       "             27,  1671,  1220,  6927,  1671, 11037,  6104,   465]),\n",
       "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]),\n",
       "  'task_type': tensor([0])},\n",
       " tensor([   0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2 Model with Adapter for Multi-Task Learning\n",
    "\n",
    "In this section, we will demonstrate how to build a parameter-efficient language model using our PELLM models for multi-task learning in a federated setting. The PELLM model already comes equipped with the Adapter mechanism, which simplifies the process of integrating multiple tasks into a single model.\n",
    "\n",
    "We will focus on implementing two tasks in our multi-task learning setup - sentiment analysis and named entity recognition (NER). The PELLM model will have two classification heads, one for each task, enabling it to simultaneously perform both tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%save_to_fate model gpt2_multitask.py\n",
    "import torch as t\n",
    "from federatedml.nn.model_zoo.pellm.gpt2 import GPT2\n",
    "\n",
    "\n",
    "class MultiTaskGPT2(GPT2):\n",
    "\n",
    "    \"\"\"MultiTaskGPT2\n",
    "    \n",
    "    Args:\n",
    "        hidden_size (int): embedding size of the GPT2 model\n",
    "        output_dim1 (int, optional): output dimension of the first task. Defaults to 2.\n",
    "        output_dim2 (int, optional): output dimension of the second task. Defaults to 9.\n",
    "        pretrained_path : pretrained model path, or use 'gpt2' to download model from huggingface\n",
    "        adapter_type: adapter type, see parent class for details\n",
    "    \"\"\"\n",
    "    # ner tag number is 9 in this conll dataset\n",
    "    def __init__(self, hidden_size, output_dim1=2, output_dim2=9, **kwargs):\n",
    "        super(MultiTaskGPT2, self).__init__(**kwargs)\n",
    "        \n",
    "        # sentimental classifcation\n",
    "        self.softmax = t.nn.Softmax(dim=-1)\n",
    "        self.classifier = t.nn.Linear(hidden_size, output_dim1)\n",
    "        # ner classification\n",
    "        self.ner_classifier = t.nn.Linear(hidden_size, output_dim2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        task_type = data['task_type']\n",
    "\n",
    "        # GPT2 forward pass\n",
    "        outputs = super().forward(data)\n",
    "\n",
    "        # Get the last hidden state from GPT2\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "        # Split the input based on the task type\n",
    "        task0_mask = task_type == 0\n",
    "        task1_mask = task_type == 1\n",
    "\n",
    "        if task0_mask.any():\n",
    "            # Classification task\n",
    "            task0_logits = self.classifier(last_hidden_state[task0_mask.flatten()][::, -1, ::])\n",
    "            task0_logits = self.softmax(task0_logits)\n",
    "\n",
    "        if task1_mask.any():\n",
    "            # Sequence labeling task\n",
    "            task1_logits = self.ner_classifier(last_hidden_state[task1_mask.flatten()])\n",
    "            task1_logits = self.softmax(task1_logits)\n",
    "\n",
    "        out_ = {0: None, 1: None, 'task_type': task_type}\n",
    "        if task0_mask.any():\n",
    "            out_[0] = task0_logits\n",
    "        if task1_mask.any():\n",
    "            out_[1] = task1_logits\n",
    "\n",
    "        return out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiTaskGPT2(hidden_size=768, pretrained_path='your path', adapter_type='HoulsbyConfig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-task Loss\n",
    "\n",
    "The MultiTaskLoss function computes a weighted sum of the losses for each task in the multi-task learning setup. The weights for each task are specified in the task_weights parameter, allowing users to adjust the relative importance of each task.\n",
    "\n",
    "See [Loss Customization](./Homo-NN-Customize-Loss.ipynb) for more details of customizing a loss class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%save_to_fate loss multi_task_loss.py\n",
    "import torch\n",
    "\n",
    "class MultiTaskLoss(torch.nn.Module):\n",
    "    def __init__(self, task_weights):\n",
    "        super().__init__()\n",
    "        self.task_weights = task_weights\n",
    "        self.classification_loss = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        self.ner_loss = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    def forward(self, model_out, labels):\n",
    "        loss = 0\n",
    "        if model_out[0] is not None:\n",
    "            # Compute classification loss\n",
    "            classification_labels = labels[model_out['task_type'].flatten() == 0]\n",
    "            classification_logits = model_out[0]\n",
    "            classification_loss = self.classification_loss(classification_logits, classification_labels[::, 0])\n",
    "            weighted_classification_loss = classification_loss * self.task_weights[0]\n",
    "            loss += weighted_classification_loss\n",
    "\n",
    "        if model_out[1] is not None:\n",
    "            # Compute NER loss\n",
    "            ner_labels = labels[model_out['task_type'].flatten() == 1].flatten()\n",
    "            ner_logits = model_out[1].reshape(-1, model_out[1].shape[-1])\n",
    "            ner_loss = self.ner_loss(ner_logits, ner_labels)\n",
    "            weighted_ner_loss = ner_loss * self.task_weights[1]\n",
    "            loss += weighted_ner_loss\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = MultiTaskLoss([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Test\n",
    "\n",
    "Before submitting a federated learning task, it is important to perform local testing to ensure that your custom dataset and model are working properly.\n",
    "\n",
    "To perform local testing with the MultiTaskFedAVGTrainer class, we will first instantiate the class with our preprocessed dataset and PELLM model with Adapter. We will then run the trainer in local mode, using a small subset of the data, to test that the model is working as expected.\n",
    "\n",
    "It is important to note that the MultiTaskFedAVGTrainer class is a toy class that has not been rigorously tested. Don't use it for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.component.nn import save_to_fate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%save_to_fate trainer multi_task_fedavg.py\n",
    "import tqdm\n",
    "import torch as t\n",
    "from torch.utils.data import DataLoader\n",
    "from federatedml.nn.homo.trainer.fedavg_trainer import FedAVGTrainer\n",
    "from federatedml.util import LOGGER\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "class MultiTaskFedAVGTrainer(FedAVGTrainer):\n",
    "\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def train_an_epoch(self, epoch_idx, model, train_set, optimizer, loss):\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        batch_idx = 0\n",
    "        acc_num = 0\n",
    "\n",
    "        if self.data_loader is None:\n",
    "            self.data_loader = DataLoader(\n",
    "                                    train_set,\n",
    "                                    batch_size=self.batch_size,\n",
    "                                    pin_memory=self.pin_memory,\n",
    "                                    shuffle=self.shuffle,\n",
    "                                    num_workers=self.data_loader_worker)\n",
    "        \n",
    "        dl = self.data_loader\n",
    "\n",
    "        if not self.fed_mode:\n",
    "            to_iterate = tqdm.tqdm(dl)\n",
    "        else:\n",
    "            to_iterate = dl\n",
    "\n",
    "        task_pred = {0: [], 1: []}\n",
    "        task_label = {0: [], 1: []}\n",
    "\n",
    "        for batch_data, batch_label in to_iterate:\n",
    "\n",
    "            if self.cuda is not None:\n",
    "                batch_data, batch_label = self.to_cuda(\n",
    "                    batch_data, self.cuda_main_device), self.to_cuda(batch_label, self.cuda_main_device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(batch_data)\n",
    "            batch_loss = loss(pred, batch_label)\n",
    "\n",
    "            if pred[0] is not None:\n",
    "                task_pred[0].append(pred[0].cpu().detach())\n",
    "                task_label[0].append(batch_label[batch_data['task_type'].flatten() == 0].cpu().detach())\n",
    "            if pred[1] is not None:\n",
    "                task_pred[1].append(pred[1].cpu().detach())\n",
    "                task_label[1].append(batch_label[batch_data['task_type'].flatten() == 1].cpu().detach())\n",
    "\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_loss_np = batch_loss.detach().numpy(\n",
    "            ) if self.cuda is None else batch_loss.cpu().detach().numpy()\n",
    "            if acc_num + self.batch_size > len(train_set):\n",
    "                batch_len = len(train_set) - acc_num\n",
    "            else:\n",
    "                batch_len = self.batch_size\n",
    "            epoch_loss += batch_loss_np * batch_len\n",
    "            batch_idx += 1\n",
    "\n",
    "        task_0_pred = t.vstack(task_pred[0]).argmax(dim=1).flatten()\n",
    "        task_0_label = t.vstack(task_label[0])[::, 0].flatten()\n",
    "        LOGGER.debug('task 0 acc {}'.format(accuracy_score(task_0_label.numpy(), task_0_pred.numpy())))\n",
    "        task_1_pred = t.vstack(task_pred[1]).argmax(dim=-1).flatten()\n",
    "        task_1_label = t.vstack(task_label[1]).flatten()\n",
    "        mask = task_1_label != -100\n",
    "        LOGGER.debug('task 1 acc {}'.format(accuracy_score(task_1_pred[mask].numpy(), task_1_label[mask].numpy())))\n",
    "\n",
    "\n",
    "        if self.fed_mode:\n",
    "            LOGGER.debug(\n",
    "                'epoch {} batch {} finished'.format(epoch_idx, batch_idx))\n",
    "        \n",
    "        epoch_loss = epoch_loss / len(train_set)\n",
    "        return epoch_loss\n",
    "    \n",
    "    def predict(self, dataset):\n",
    "        # currently FATE does not support handling the result of multi-task model, so\n",
    "        # we disable the predict function\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MultiTaskFedAVGTrainer(epochs=5, batch_size=4, data_loader_worker=8, shuffle=False)\n",
    "trainer.local_mode()\n",
    "trainer.set_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch is 0\n",
      "100%|██████████| 250/250 [02:15<00:00,  1.84it/s]\n",
      "task 0 acc 0.514\n",
      "task 1 acc 0.7738525357955306\n",
      "epoch loss is 0.6051078315377235\n",
      "epoch is 1\n",
      "100%|██████████| 250/250 [02:22<00:00,  1.76it/s]\n",
      "task 0 acc 0.52\n",
      "task 1 acc 0.83701324769169\n",
      "epoch loss is 0.5854948361515999\n",
      "epoch is 2\n",
      "100%|██████████| 250/250 [02:19<00:00,  1.80it/s]\n",
      "task 0 acc 0.522\n",
      "task 1 acc 0.83701324769169\n",
      "epoch loss is 0.584037158548832\n",
      "epoch is 3\n",
      "100%|██████████| 250/250 [02:23<00:00,  1.74it/s]\n",
      "task 0 acc 0.524\n",
      "task 1 acc 0.83701324769169\n",
      "epoch loss is 0.582937289237976\n",
      "epoch is 4\n",
      "100%|██████████| 250/250 [02:43<00:00,  1.53it/s]\n",
      "task 0 acc 0.526\n",
      "task 1 acc 0.83701324769169\n",
      "epoch loss is 0.5806194019317626\n"
     ]
    }
   ],
   "source": [
    "optimizer = t.optim.Adam(model.parameters(), lr=0.0001)\n",
    "trainer.train(ds, None, optimizer, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Federated Task\n",
    "Once you have successfully completed local testing, We can submit a task to FATE. Please notice that this tutorial is ran on a standalone version. **Please notice that in this tutorial we are using a standalone version, if you are using a cluster version, you need to bind the data with the corresponding name&namespace on each machine.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-03-31 11:28:05.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mJob id is 202303311128051297170\n",
      "\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:05.428\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m98\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KJob is still waiting, time elapse: 0:00:00\u001b[0m\n",
      "\u001b[0mm2023-03-31 11:28:06.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1m\n",
      "\u001b[32m2023-03-31 11:28:06.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component reader_0, time elapse: 0:00:01\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:07.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component reader_0, time elapse: 0:00:02\u001b[0m\n",
      "\u001b[0mm2023-03-31 11:28:08.630\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1m\n",
      "\u001b[32m2023-03-31 11:28:08.631\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:03\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:09.694\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:04\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:10.707\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:05\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:11.720\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:06\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:12.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:07\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:13.745\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:08\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:14.792\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:09\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:15.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:10\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:16.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:11\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:17.926\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:12\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:18.965\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:13\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:20.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:14\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:21.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:15\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:22.121\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:16\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:23.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:17\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:24.183\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:18\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:25.216\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:19\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:26.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:20\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:27.330\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:21\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:28.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:22\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:29.427\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:24\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:30.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:25\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:31.563\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:26\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:32.621\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:27\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:33.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:28\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:34.694\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:29\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:35.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:30\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:36.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:31\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:37.865\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:32\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:38.899\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:33\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:39.985\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:34\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:41.016\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:35\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:42.079\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:36\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:43.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:37\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:44.204\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:38\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:45.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:39\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:46.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:40\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:47.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:41\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:48.271\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:42\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:49.283\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:43\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:50.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:44\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:51.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:45\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:52.332\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:46\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:53.344\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:47\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:54.357\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:48\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:55.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:49\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:56.382\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:50\u001b[0m\n",
      "\u001b[32m2023-03-31 11:28:57.399\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:51\u001b[0m\n",
      "\u001b[32m2023-03-31 11:29:01.515\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:56\u001b[0m\n",
      "\u001b[32m2023-03-31 11:29:10.392\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:01:04\u001b[0m\n",
      "\u001b[32m2023-03-31 11:29:11.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:01:06\u001b[0m\n",
      "\u001b[32m2023-03-31 11:29:12.438\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:01:07\u001b[0m\n",
      "\u001b[32m2023-03-31 11:29:14.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1mJob is success!!! Job id is 202303311128051297170\u001b[0m\n",
      "\u001b[32m2023-03-31 11:29:14.820\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mTotal time: 0:01:09\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import os\n",
    "from pipeline import fate_torch_hook\n",
    "from pipeline.component import HomoNN\n",
    "from pipeline.backend.pipeline import PipeLine\n",
    "from pipeline.component import Reader\n",
    "from pipeline.interface import Data\n",
    "\n",
    "fate_torch_hook(t)\n",
    "\n",
    "\n",
    "fate_project_path = os.path.abspath('../../../../')\n",
    "guest_0 = 10000\n",
    "host_1 = 9999\n",
    "pipeline = PipeLine().set_initiator(role='guest', party_id=guest_0).set_roles(guest=guest_0, host=host_1,\n",
    "                                                                              arbiter=guest_0)\n",
    "data_0 = {\"name\": \"imdb\", \"namespace\": \"experiment\"}\n",
    "data_path = fate_project_path + '/doc/tutorial/pipeline/nn_tutorial'\n",
    "pipeline.bind_table(name=data_0['name'], namespace=data_0['namespace'], path=data_path)\n",
    "pipeline.bind_table(name=data_0['name'], namespace=data_0['namespace'], path=data_path)\n",
    "reader_0 = Reader(name=\"reader_0\")\n",
    "reader_0.get_party_instance(role='guest', party_id=guest_0).component_param(table=data_0)\n",
    "reader_0.get_party_instance(role='host', party_id=host_1).component_param(table=data_0)\n",
    "\n",
    "reader_1 = Reader(name=\"reader_1\")\n",
    "reader_1.get_party_instance(role='guest', party_id=guest_0).component_param(table=data_0)\n",
    "reader_1.get_party_instance(role='host', party_id=host_1).component_param(table=data_0)\n",
    "\n",
    "\n",
    "## Add your pretriained model path here, will load model&tokenizer from this path\n",
    "model_path = ''\n",
    "\n",
    "\n",
    "from pipeline.component.homo_nn import DatasetParam, TrainerParam  \n",
    "model = t.nn.Sequential(\n",
    "    t.nn.CustModel(module_name='gpt2_multitask', class_name='MultiTaskGPT2', pretrained_path=model_path, adapter_type='HoulsbyConfig', hidden_size=768)\n",
    ")\n",
    "\n",
    "# DatasetParam\n",
    "dataset_param = DatasetParam(dataset_name='multitask_ds', take_limits=50, tokenizer_name_or_path=model_path)\n",
    "# TrainerParam\n",
    "trainer_param = TrainerParam(trainer_name='multi_task_fedavg', epochs=1, batch_size=4, \n",
    "                             data_loader_worker=8, secure_aggregate=True)\n",
    "loss = t.nn.CustLoss(loss_module_name='multi_task_loss', class_name='MultiTaskLoss', task_weights=[0.5, 0.5])\n",
    "\n",
    "\n",
    "nn_component = HomoNN(name='nn_0', model=model)\n",
    "\n",
    "# set parameter for client 1\n",
    "nn_component.get_party_instance(role='guest', party_id=guest_0).component_param(\n",
    "    loss=loss,\n",
    "    optimizer = t.optim.Adam(lr=0.0001, eps=1e-8),\n",
    "    dataset=dataset_param,       \n",
    "    trainer=trainer_param,\n",
    "    torch_seed=100 \n",
    ")\n",
    "\n",
    "# set parameter for client 2\n",
    "nn_component.get_party_instance(role='host', party_id=host_1).component_param(\n",
    "    loss=loss,\n",
    "    optimizer = t.optim.Adam(lr=0.0001, eps=1e-8),\n",
    "    dataset=dataset_param,       \n",
    "    trainer=trainer_param,\n",
    "    torch_seed=100 \n",
    ")\n",
    "\n",
    "# set parameter for server\n",
    "nn_component.get_party_instance(role='arbiter', party_id=guest_0).component_param(    \n",
    "    trainer=trainer_param\n",
    ")\n",
    "\n",
    "pipeline.add_component(reader_0)\n",
    "pipeline.add_component(nn_component, data=Data(train_data=reader_0.output.data))\n",
    "pipeline.compile()\n",
    "\n",
    "pipeline.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
