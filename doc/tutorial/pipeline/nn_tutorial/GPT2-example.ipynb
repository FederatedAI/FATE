{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Federated GPT-2 Tuning with Parameter Efficient methods in FATE-1.11"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will demonstrate how to efficiently train federated large language models using the FATE 1.11 framework. In FATE-1.11, we introduce the \"pellm\"(Parameter Efficient Large Language Model) module, specifically designed for federated learning with large language models. We enable the implementation of parameter-efficient methods in federated learning, reducing communication overhead while maintaining model performance. In this tutorial we particularlly focus on GPT-2, and we will also emphasize the use of the Adapter mechanism for fine-tuning GPT-2, which enables us to effectively reduce communication volume and improve overall efficiency.\n",
    "\n",
    "By following this tutorial, you will learn how to leverage the FATE framework to rapidly fine-tune federated large language models, such as GPT-2, with ease and efficiency."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2\n",
    "\n",
    "GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. GPT-2 is trained with a causal language modeling (CLM) objective, conditioning on a left-to-right context window of 1024 tokens. In this tutorial, we will use GPT2, you can download the pretrained model from [here](https://huggingface.co/gpt2) (We choose the smallest version for this tutorial), or let the program automatically download it when you use it later."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: IMDB Sentimental\n",
    "\n",
    "In this section, we will introduce the process of preparing the IMDB dataset for use in our federated learning task. We use our tokenizer dataset(based on HuggingFace tokenizer) to preprocess the text data.\n",
    "\n",
    "About IMDB Sentimental Dataset:\n",
    "\n",
    "This is an binary classification dataset, you can download our processed dataset from here: \n",
    "- https://webank-ai-1251170195.cos.ap-guangzhou.myqcloud.com/fate/examples/data/IMDB.csv\n",
    "and place it in the examples/data folder. \n",
    "\n",
    "The orgin data is from: \n",
    "- https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "### Check Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../../../../examples/data/IMDB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996</td>\n",
       "      <td>THE CELL (2000) Rating: 8/10&lt;br /&gt;&lt;br /&gt;The Ce...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997</td>\n",
       "      <td>This movie, despite its list of B, C, and D li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998</td>\n",
       "      <td>I loved this movie! It was all I could do not ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999</td>\n",
       "      <td>This was the worst movie I have ever seen Bill...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>2000</td>\n",
       "      <td>Stranded in Space (1972) MST3K version - a ver...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2001 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  label\n",
       "0        0  One of the other reviewers has mentioned that ...      1\n",
       "1        1  A wonderful little production. <br /><br />The...      1\n",
       "2        2  I thought this was a wonderful way to spend ti...      1\n",
       "3        3  Basically there's a family where a little boy ...      0\n",
       "4        4  Petter Mattei's \"Love in the Time of Money\" is...      1\n",
       "...    ...                                                ...    ...\n",
       "1996  1996  THE CELL (2000) Rating: 8/10<br /><br />The Ce...      1\n",
       "1997  1997  This movie, despite its list of B, C, and D li...      0\n",
       "1998  1998  I loved this movie! It was all I could do not ...      1\n",
       "1999  1999  This was the worst movie I have ever seen Bill...      0\n",
       "2000  2000  Stranded in Space (1972) MST3K version - a ver...      0\n",
       "\n",
       "[2001 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from federatedml.nn.dataset.nlp_tokenizer import TokenizerDataset\n",
    "ds = TokenizerDataset(tokenizer_name_or_path=\"gpt2\", text_max_length=128, \n",
    "                      padding_side=\"left\", return_input_ids=False, pad_token='<|endoftext|>')  # you can load tokenizer config from local pretrained tokenizer\n",
    "ds.load('../../../../examples/data/IMDB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([ 3198,   286,   262,   584, 30702,   468,  4750,   326,   706,  4964,\n",
       "            655,   352, 18024,  4471,   345,  1183,   307, 23373,    13,  1119,\n",
       "            389,   826,    11,   355,   428,   318,  3446,   644,  3022,   351,\n",
       "            502, 29847,  1671,  1220,  6927,  1671, 11037,   464,   717,  1517,\n",
       "            326,  7425,   502,   546, 18024,   373,   663, 24557,   290, 42880,\n",
       "           8589,   278,  8188,   286,  3685,    11,   543,   900,   287,   826,\n",
       "            422,   262,  1573, 10351,    13,  9870,   502,    11,   428,   318,\n",
       "            407,   257,   905,   329,   262, 18107,  2612,   276,   393, 44295,\n",
       "             13,   770,   905, 16194,   645, 25495,   351, 13957,   284,  5010,\n",
       "             11,  1714,   393,  3685,    13,  6363,   318, 22823,    11,   287,\n",
       "            262,  6833,   779,   286,   262,  1573, 29847,  1671,  1220,  6927,\n",
       "           1671, 11037,  1026,   318,  1444,   440,    57,   355,   326,   318,\n",
       "            262, 21814,  1813,   284,   262, 34374, 22246,  4765]),\n",
       "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1])},\n",
       " array([1.], dtype=float32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details of FATE dataset setting, we recommend that you read through these tutorials first: [NN Dataset Customization](./Homo-NN-Customize-your-Dataset.ipynb), [Some Built-In Dataset](./Introduce-Built-In-Dataset.ipynb),"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PELLM Model with Adapter\n",
    "\n",
    "In this section, we will guide you through the process of building a parameter-efficient language model using the FATE framework. We will focus on the implementation of the PELLM model and the integration of the Adapter mechanism, which enables efficient fine-tuning and reduces communication overhead in federated learning settings. Take GPT-2 as example you will learn how to leverage the FATE framework to rapidly develop and deploy a parameter-efficient language model using FATE built-in classes. Before starting this section, we recommend that you read through this tutorial first: [Model Customization](./Homo-NN-Customize-Model.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PELLM Models\n",
    "\n",
    "In this section we introduce the PELLM model, which is a parameter-efficient language model that can be used in federated learning settings. They are designed to be compatible with the FATE framework to enable federated model tuning/training.\n",
    "\n",
    "PELLM models are located at federatedml.nn.model_zoo.pellm(federatedml/nn/model_zoo/pellm):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "albert.py  bert.py     distillbert.py  parameter_efficient_llm.py  roberta.py\n",
      "bart.py    deberta.py  gpt2.py\t       __pycache__\n"
     ]
    }
   ],
   "source": [
    "! ls ../../../../fate/python/federatedml/nn/model_zoo/pellm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can initialize your GPT2 model with corresponding model config dict, or you can load the pretrained model from the model folder, or download the pretrained model from the Huggingface,\n",
    "here we initialize the GPT2 model with the Houlsby Adapter, we will introduce Adapters in the following sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from federatedml.nn.model_zoo.pellm.gpt2 import GPT2\n",
    "\n",
    "# case 1 initialize with config dict \n",
    "from transformers import GPT2Config\n",
    "gpt2 = GPT2(config=GPT2Config().to_dict(), adapter_type='HoulsbyConfig')\n",
    "\n",
    "# case 2 load pretrained weights from local pretrained weights, it is the same as using the huggingface pretrained model\n",
    "path_to_pretrained_folder = ''\n",
    "gpt2 = GPT2(pretrained_path=path_to_pretrained_folder, adapter_type='HoulsbyConfig')\n",
    "\n",
    "# case 3 directly download models from huggingface\n",
    "gpt2 = GPT2(pretrained_path='gpt2', adapter_type='HoulsbyConfig')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version we currently support these language model for federated training:\n",
    "- Bert\n",
    "- ALBert\n",
    "- RoBerta\n",
    "- GPT-2\n",
    "- Bart\n",
    "- DeBerta\n",
    "- DistillBert"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, you can use auto model(similar to automodel in huggingface) to automatically import models from your local pretrained model folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from federatedml.nn.model_zoo.pellm.parameter_efficient_llm import AutoPELLM\n",
    "\n",
    "path_to_pretrained_folder = ''\n",
    "gpt2 = AutoPELLM(pretrained_path=path_to_pretrained_folder, adapter_type='HoulsbyConfig')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapters\n",
    "\n",
    "We can directly use adapters from the adapterhub. See details for adapters on this page [Adapter Methods](https://docs.adapterhub.ml/overview.html) for more details. By specifying the adapter name and the adapter\n",
    "config dict we can insert adapters into our language models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.adapters import HoulsbyConfig\n",
    "path_to_pretrained_folder = ''\n",
    "# use default adapter setting\n",
    "gpt2 = GPT2(pretrained_path=path_to_pretrained_folder, adapter_type='HoulsbyConfig', adapter_config=HoulsbyConfig().to_dict())\n",
    "# set adapter parameters\n",
    "gpt2 = GPT2(pretrained_path=path_to_pretrained_folder, adapter_type='HoulsbyConfig', adapter_config=HoulsbyConfig(init_weights='mam_adapter').to_dict())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**During the training process, all weights of the pretrained language model will be frozen, and weights of adapters are traininable. Thus, FATE only train in the local training and aggregate adapters' weights in the fedederation process**\n",
    "\n",
    "Now available adapters are [Adapters Overview](https://docs.adapterhub.ml/overview.html) for details.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use PELLM Model in FATE with CustModel\n",
    "\n",
    "In this [Model Customization](./Homo-NN-Customize-Model.ipynb) tutorial, we demonstrate how to employ the t.nn.CustomModel class in fate_torch to parse a model's structure and submit it to a federated learning task. The CustomModel automatically imports the model class from the model_zoo and initializes the models with the parameters provided. Since these language models are built-in, we can directly use them in the CustomModel and easily add a classifier head to address the classification task at hand："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch' from '/home/cwj/FATE-standalone/standalone_fate_install_1.10.0_release/env/python/venv/lib/python3.8/site-packages/torch/__init__.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t\n",
    "from pipeline import fate_torch_hook\n",
    "from pipeline.component.nn import save_to_fate\n",
    "fate_torch_hook(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%save_to_fate model classifier_head.py\n",
    "\n",
    "import torch as t\n",
    "\n",
    "class Classifier(t.nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=768):\n",
    "        super().__init__()\n",
    "        self.linr = t.nn.Linear(in_size, 1)\n",
    "        self.sigmoid = t.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.last_hidden_state[::, -1]\n",
    "        return self.sigmoid(self.linr(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cwj/FATE-standalone/standalone_fate_install_1.10.0_release/env/python/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# build CustModel with PELLM, and add a classifier head\n",
    "from transformers import GPT2Config\n",
    "model = t.nn.Sequential(\n",
    "    t.nn.CustModel(module_name='pellm.gpt2', class_name='GPT2', config=GPT2Config().to_dict(), adapter_type='HoulsbyConfig'),\n",
    "    t.nn.CustModel(module_name='classifier_head', class_name='Classifier', in_size=768)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Please note that during the training process, the classifier parameters will be trained and sent to the server for aggregation. In fact, all trainable parameters will participate in the federated learning process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Test\n",
    "\n",
    "Before submitting a federated learning task, we will demonstrate how to perform local testing to ensure the proper functionality of your custom dataset, model. We use the local mode of our FedAVGTrainer to test if our setting can run correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from model config dict\n",
      "PELM model summary: \n",
      "================================================================================\n",
      "Name                     Architecture         #Param      %Param  Active   Train\n",
      "--------------------------------------------------------------------------------\n",
      "federation               bottleneck        1,789,056       1.438       1       1\n",
      "--------------------------------------------------------------------------------\n",
      "Full model                               124,439,808     100.000               0\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from federatedml.nn.model_zoo.pellm.gpt2 import GPT2\n",
    "from federatedml.nn.homo.trainer.fedavg_trainer import FedAVGTrainer\n",
    "from transformers import GPT2Config\n",
    "from federatedml.nn.dataset.nlp_tokenizer import TokenizerDataset\n",
    "\n",
    "# load dataset\n",
    "ds = TokenizerDataset(tokenizer_name_or_path=\"gpt2\", text_max_length=128, \n",
    "                      padding_side=\"left\", return_input_ids=False, pad_token='<|endoftext|>')  # you can load tokenizer config from local pretrained tokenizer\n",
    "ds.load('../../../../examples/data/IMDB.csv')\n",
    "\n",
    "model = t.nn.Sequential(\n",
    "    GPT2(config=GPT2Config().to_dict(), adapter_type='HoulsbyConfig'),\n",
    "    Classifier(768)\n",
    ")\n",
    "\n",
    "trainer = FedAVGTrainer(epochs=1, batch_size=8, shuffle=True, data_loader_worker=8)\n",
    "trainer.local_mode()\n",
    "trainer.set_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch is 0\n",
      "100%|██████████| 251/251 [07:11<00:00,  1.72s/it]\n",
      "epoch loss is 0.7566220122894486\n"
     ]
    }
   ],
   "source": [
    "opt = t.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss = t.nn.BCELoss()\n",
    "# local test, here we only use CPU for training\n",
    "trainer.train(ds, None, opt, loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Federated Task\n",
    "Once you have successfully completed local testing, We can submit a task to FATE. Please notice that this tutorial is ran on a standalone version. **Please notice that in this tutorial we are using a standalone version, if you are using a cluster version, you need to bind the data with the corresponding name&namespace on each machine.**\n",
    "\n",
    "In this example we load pretrained weights for gpt2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pipeline.backend.pipeline.PipeLine at 0x7f43f11b8820>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t\n",
    "import os\n",
    "from pipeline import fate_torch_hook\n",
    "from pipeline.component import HomoNN\n",
    "from pipeline.backend.pipeline import PipeLine\n",
    "from pipeline.component import Reader\n",
    "from pipeline.interface import Data\n",
    "from transformers import GPT2Config\n",
    "\n",
    "fate_torch_hook(t)\n",
    "\n",
    "\n",
    "fate_project_path = os.path.abspath('../../../../')\n",
    "guest_0 = 10000\n",
    "host_1 = 9999\n",
    "pipeline = PipeLine().set_initiator(role='guest', party_id=guest_0).set_roles(guest=guest_0, host=host_1,\n",
    "                                                                              arbiter=guest_0)\n",
    "data_0 = {\"name\": \"imdb\", \"namespace\": \"experiment\"}\n",
    "data_path = fate_project_path + '/examples/data/IMDB.csv'\n",
    "pipeline.bind_table(name=data_0['name'], namespace=data_0['namespace'], path=data_path)\n",
    "pipeline.bind_table(name=data_0['name'], namespace=data_0['namespace'], path=data_path)\n",
    "reader_0 = Reader(name=\"reader_0\")\n",
    "reader_0.get_party_instance(role='guest', party_id=guest_0).component_param(table=data_0)\n",
    "reader_0.get_party_instance(role='host', party_id=host_1).component_param(table=data_0)\n",
    "\n",
    "reader_1 = Reader(name=\"reader_1\")\n",
    "reader_1.get_party_instance(role='guest', party_id=guest_0).component_param(table=data_0)\n",
    "reader_1.get_party_instance(role='host', party_id=host_1).component_param(table=data_0)\n",
    "\n",
    "\n",
    "## Add your pretriained model path here, will load model&tokenizer from this path\n",
    "model_path = ''\n",
    "\n",
    "\n",
    "from pipeline.component.homo_nn import DatasetParam, TrainerParam  \n",
    "model = t.nn.Sequential(\n",
    "    t.nn.CustModel(module_name='pellm.gpt2', class_name='GPT2', config=GPT2Config().to_dict(), adapter_type='HoulsbyConfig'),\n",
    "    t.nn.CustModel(module_name='classifier_head', class_name='Classifier', in_size=768)\n",
    ")\n",
    "\n",
    "# DatasetParam\n",
    "dataset_param = DatasetParam(dataset_name='nlp_tokenizer',text_max_length=128, tokenizer_name_or_path=model_path, \n",
    "                             padding_side=\"left\", return_input_ids=False, pad_token='<|endoftext|>')\n",
    "# TrainerParam\n",
    "trainer_param = TrainerParam(trainer_name='fedavg_trainer', epochs=1, batch_size=8, \n",
    "                             data_loader_worker=8, secure_aggregate=False)\n",
    "\n",
    "\n",
    "nn_component = HomoNN(name='nn_0', model=model)\n",
    "\n",
    "# set parameter for client 1\n",
    "nn_component.get_party_instance(role='guest', party_id=guest_0).component_param(\n",
    "    loss=t.nn.BCELoss(),\n",
    "    optimizer = t.optim.Adam(lr=0.0001, eps=1e-8),\n",
    "    dataset=dataset_param,       \n",
    "    trainer=trainer_param,\n",
    "    torch_seed=100 \n",
    ")\n",
    "\n",
    "# set parameter for client 2\n",
    "nn_component.get_party_instance(role='host', party_id=host_1).component_param(\n",
    "    loss=t.nn.BCELoss(),\n",
    "    optimizer = t.optim.Adam(lr=0.0001, eps=1e-8),\n",
    "    dataset=dataset_param,       \n",
    "    trainer=trainer_param,\n",
    "    torch_seed=100 \n",
    ")\n",
    "\n",
    "# set parameter for server\n",
    "nn_component.get_party_instance(role='arbiter', party_id=guest_0).component_param(    \n",
    "    trainer=trainer_param\n",
    ")\n",
    "\n",
    "pipeline.add_component(reader_0)\n",
    "pipeline.add_component(nn_component, data=Data(train_data=reader_0.output.data))\n",
    "pipeline.compile()\n",
    "\n",
    "pipeline.fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this script to submit the model, but submitting the model will take a long time to train and generate a long log, so we won't do it here."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with CUDA\n",
    "\n",
    "You can use GPU by setting the cuda parameter of the FedAVGTrainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_param = TrainerParam(trainer_name='fedavg_trainer', epochs=1, batch_size=8, \n",
    "                             data_loader_worker=8, secure_aggregate=False, cuda=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cuda parameter here accepts an integer value that corresponds to the index of the GPU you want to use for training. \n",
    "In the example above, the value is set to 0, which means that on every client the first available GPU in the system will be used. \n",
    "If you have multiple GPUs and would like to use a specific one, simply change the value of the cuda parameter to the appropriate index."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with multiple GPUs\n",
    "\n",
    "To take advantage of multiple GPUs, you can assign different GPUs to different clients during federated learning. In the example below, we configure two clients to use different sets of GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_0_param = TrainerParam(trainer_name='fedavg_trainer', epochs=1, batch_size=8, \n",
    "                             data_loader_worker=8, secure_aggregate=False, cuda=[0, 1, 2, 3])\n",
    "client_1_param = TrainerParam(trainer_name='fedavg_trainer', epochs=1, batch_size=8, \n",
    "                             data_loader_worker=8, secure_aggregate=False, cuda=[0, 3, 4])\n",
    "server_param = TrainerParam(trainer_name='fedavg_trainer', epochs=1, batch_size=8, \n",
    "                             data_loader_worker=8, secure_aggregate=False)\n",
    "\n",
    "# set parameter for client 1\n",
    "nn_component.get_party_instance(role='guest', party_id=guest_0).component_param(\n",
    "    loss=t.nn.BCELoss(),\n",
    "    optimizer = t.optim.Adam(lr=0.0001, eps=1e-8),\n",
    "    dataset=dataset_param,       \n",
    "    trainer=client_0_param,\n",
    "    torch_seed=100 \n",
    ")\n",
    "\n",
    "# set parameter for client 2\n",
    "nn_component.get_party_instance(role='host', party_id=host_1).component_param(\n",
    "    loss=t.nn.BCELoss(),\n",
    "    optimizer = t.optim.Adam(lr=0.0001, eps=1e-8),\n",
    "    dataset=dataset_param,       \n",
    "    trainer=client_1_param,\n",
    "    torch_seed=100 \n",
    ")\n",
    "\n",
    "# set parameter for server\n",
    "nn_component.get_party_instance(role='arbiter', party_id=guest_0).component_param(    \n",
    "    trainer=server_param\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, client_0 is set to use GPUs with indices [0, 1, 2, 3], while client_1 uses GPUs with indices [0, 3, 4]. The server does not support GPUs usage in the aggregation procession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
