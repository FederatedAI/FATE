{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee4aa0e",
   "metadata": {},
   "source": [
    "# Homo NN 单机版快速开始"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deff6c23",
   "metadata": {},
   "source": [
    "在该版本中， pipeline中Homo-NN加入了对pytorch的支持，可以遵照pytorch往常的使用方式，定义Sequential模型，使用torch自带的layers, 提交模型\n",
    "\n",
    "下面给出一个基础的二分类任务Homo-NN任务，有两个client，party id分别为10000，9999参与，并指定10000为server端聚合模型。\n",
    "\n",
    "使用方法与的其他FATE算法一致：使用FATE自带的reader, transformer接口进行表格数据输入，\n",
    "数据输入到算法组件中，组件使用定义的模型，优化器和loss函数进行训练，聚合模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fd3f74",
   "metadata": {},
   "source": [
    "## 上传csv数据到FATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611ca3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " UPLOADING:||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2022-11-03 12:02:40.643\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mJob id is 202211031202403640850\n",
      "\u001b[0m\n",
      "\u001b[32m2022-11-03 12:02:40.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m98\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KJob is still waiting, time elapse: 0:00:00\u001b[0m\n",
      "\u001b[32m2022-11-03 12:02:41.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m98\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KJob is still waiting, time elapse: 0:00:01\u001b[0m\n",
      "\u001b[0mm2022-11-03 12:02:42.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1m\n",
      "\u001b[32m2022-11-03 12:02:42.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component upload_0, time elapse: 0:00:02\u001b[0m\n",
      "\u001b[32m2022-11-03 12:02:43.814\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component upload_0, time elapse: 0:00:03\u001b[0m\n",
      "\u001b[32m2022-11-03 12:02:44.861\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component upload_0, time elapse: 0:00:04\u001b[0m\n",
      "\u001b[32m2022-11-03 12:02:45.902\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component upload_0, time elapse: 0:00:05\u001b[0m\n",
      "\u001b[32m2022-11-03 12:02:48.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1mJob is success!!! Job id is 202211031202403640850\u001b[0m\n",
      "\u001b[32m2022-11-03 12:02:48.024\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mTotal time: 0:00:07\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pipeline.backend.pipeline import PipeLine  # pipeline类\n",
    "\n",
    "host_0 = 10000\n",
    "host_1 = 9999\n",
    "pipeline_upload = PipeLine().set_initiator(role='host', party_id=host_0).set_roles(host=[host_0, host_1],\n",
    "                                                                            arbiter=[host_0])\n",
    "\n",
    "partition = 4\n",
    "\n",
    "# 上传一份数据\n",
    "data = {\"name\": \"breast_homo_host\", \"namespace\": \"experiment\"}\n",
    "pipeline_upload.add_upload_data(file=\"./examples/data/breast_homo_host.csv\", # 以project文件夹为根目录\n",
    "                                table_name=data[\"name\"],             # table name\n",
    "                                namespace=data[\"namespace\"],         # namespace\n",
    "                                head=1, partition=partition)               # data info\n",
    "\n",
    "pipeline_upload.upload(drop=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e6fc84",
   "metadata": {},
   "source": [
    "## 编写Pipeline脚本并执行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5ccfe0",
   "metadata": {},
   "source": [
    "### import 相关组件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4eec107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch\n",
    "import torch as t\n",
    "from torch import nn\n",
    "\n",
    "# pipeline\n",
    "from pipeline.component.homo_nn import HomoNN, TrainerParam  # HomoNN组件，训练器参数\n",
    "from pipeline.backend.pipeline import PipeLine  # pipeline类\n",
    "from pipeline.component import Reader, DataTransform, Evaluation # 数据IO， Evaluation\n",
    "from pipeline.interface import Data  # Data接口，用于数据IO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48edc92d",
   "metadata": {},
   "source": [
    "### fate torch hook\n",
    "\n",
    "请务必执行下面的fate_torch_hook函数，它会够修改torch的一些类，使得你定义的torch神经网络，优化器，loss function能够被pipeline解析并提交"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "955db238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch' from '/home/cwj/standalone_fate_install_1.9.0_release/env/python/venv/lib/python3.8/site-packages/torch/__init__.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pipeline import fate_torch_hook\n",
    "fate_torch_hook(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5be800",
   "metadata": {},
   "source": [
    "### pipeline脚本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc9a174a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2022-11-03 12:02:49.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mJob id is 202211031202481083070\n",
      "\u001b[0m\n",
      "\u001b[32m2022-11-03 12:02:49.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m98\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KJob is still waiting, time elapse: 0:00:00\u001b[0m\n",
      "\u001b[0mm2022-11-03 12:02:50.167\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1m\n",
      "\u001b[32m2022-11-03 12:02:50.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component reader_0, time elapse: 0:00:01\u001b[0m\n",
      "\u001b[32m2022-11-03 12:02:51.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component reader_0, time elapse: 0:00:02\u001b[0m\n",
      "\u001b[32m2022-11-03 12:02:52.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component reader_0, time elapse: 0:00:03\u001b[0m\n",
      "\u001b[32m2022-11-03 12:02:53.279\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component reader_0, time elapse: 0:00:04\u001b[0m\n",
      "\u001b[32m2022-11-03 12:02:54.315\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component reader_0, time elapse: 0:00:05\u001b[0m\n",
      "\u001b[32m2022-11-03 12:02:55.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component reader_0, time elapse: 0:00:06\u001b[0m\n",
      "\u001b[32m2022-11-03 12:02:56.415\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component reader_0, time elapse: 0:00:07\u001b[0m\n",
      "\u001b[0mm2022-11-03 12:02:57.455\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1m\n",
      "\u001b[32m2022-11-03 12:02:57.456\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component data_transform_0, time elapse: 0:00:08\u001b[0m\n",
      "\u001b[32m2022-11-03 12:02:58.485\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component data_transform_0, time elapse: 0:00:09\u001b[0m\n",
      "\u001b[32m2022-11-03 12:02:59.539\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component data_transform_0, time elapse: 0:00:10\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:00.576\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component data_transform_0, time elapse: 0:00:11\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:01.610\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component data_transform_0, time elapse: 0:00:12\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:02.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component data_transform_0, time elapse: 0:00:13\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:03.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component data_transform_0, time elapse: 0:00:14\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:04.810\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component data_transform_0, time elapse: 0:00:15\u001b[0m\n",
      "\u001b[0mm2022-11-03 12:03:05.856\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1m\n",
      "\u001b[32m2022-11-03 12:03:05.858\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:16\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:06.894\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:17\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:07.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:18\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:09.027\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:20\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:10.059\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:21\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:11.090\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:22\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:12.118\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:23\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:13.153\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:24\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:14.221\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:25\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:15.347\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:26\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:16.373\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:27\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:17.403\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:28\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:18.434\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:29\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:19.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:30\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:20.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:31\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:21.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:32\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:22.798\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:33\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:23.947\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:34\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:25.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:35\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:26.060\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:37\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2022-11-03 12:03:27.156\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:38\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:28.211\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:39\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:29.290\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:40\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:30.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:41\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:31.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:42\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:32.563\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:43\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:33.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:44\u001b[0m\n",
      "\u001b[0mm2022-11-03 12:03:34.654\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1m\n",
      "\u001b[32m2022-11-03 12:03:34.655\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component eval_0, time elapse: 0:00:45\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:35.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component eval_0, time elapse: 0:00:46\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:36.739\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component eval_0, time elapse: 0:00:47\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:37.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component eval_0, time elapse: 0:00:48\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:38.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component eval_0, time elapse: 0:00:49\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:39.875\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component eval_0, time elapse: 0:00:50\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:40.966\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component eval_0, time elapse: 0:00:51\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:41.994\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component eval_0, time elapse: 0:00:52\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:43.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component eval_0, time elapse: 0:00:54\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:44.067\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component eval_0, time elapse: 0:00:55\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:45.114\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component eval_0, time elapse: 0:00:56\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:47.203\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1mJob is success!!! Job id is 202211031202481083070\u001b[0m\n",
      "\u001b[32m2022-11-03 12:03:47.204\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mTotal time: 0:00:58\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 创建pipeline \n",
    "host_0 = 10000\n",
    "host_1 = 9999\n",
    "pipeline = PipeLine().set_initiator(role='host', party_id=host_0).set_roles(host=[host_0, host_1],\n",
    "                                                                            arbiter=[host_0])\n",
    "\n",
    "# 设置上传数据\n",
    "train_data_0 = {\"name\": \"breast_homo_host\", \"namespace\": \"experiment\"}\n",
    "train_data_1 = {\"name\": \"breast_homo_host\", \"namespace\": \"experiment\"}\n",
    "reader_0 = Reader(name=\"reader_0\")\n",
    "reader_0.get_party_instance(role='host', party_id=host_0).component_param(table=train_data_0)\n",
    "reader_0.get_party_instance(role='host', party_id=host_1).component_param(table=train_data_1)\n",
    "\n",
    "# tranform组件将上传的数据转换为FATE标准格式\n",
    "data_transform_0 = DataTransform(name='data_transform_0')\n",
    "data_transform_0.get_party_instance(\n",
    "    role='host', party_id=host_0).component_param(\n",
    "    with_label=True, output_format=\"dense\")\n",
    "data_transform_0.get_party_instance(\n",
    "    role='host', party_id=host_1).component_param(\n",
    "    with_label=True, output_format=\"dense\")\n",
    "\n",
    "\"\"\"\n",
    "定义模型\n",
    "\"\"\"\n",
    "# 与本地使用torch sequential一致\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(30, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "loss = nn.BCELoss()\n",
    "optimizer = t.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "创建组件\n",
    "\"\"\"\n",
    "nn_component = HomoNN(name='nn_0',\n",
    "                      model=model,\n",
    "                      loss=loss,\n",
    "                      optimizer=optimizer,\n",
    "                      # TrainerParam传递参数到fedavg_trainer，关于Trainer详细请见下文\n",
    "                      trainer=TrainerParam(trainer_name='fedavg_trainer', epochs=20, batch_size=128, validation_freqs=1),\n",
    "                      torch_seed=100 # 全局随机种子\n",
    "                      )\n",
    "\n",
    "# 加入组件，定义数据间的数据IO\n",
    "pipeline.add_component(reader_0)\n",
    "pipeline.add_component(data_transform_0, data=Data(data=reader_0.output.data))\n",
    "pipeline.add_component(nn_component, data=Data(train_data=data_transform_0.output.data))\n",
    "pipeline.add_component(Evaluation(name='eval_0'), data=Data(data=nn_component.output.data))\n",
    "\n",
    "pipeline.compile()\n",
    "pipeline.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af94b45d",
   "metadata": {},
   "source": [
    "## 获取模型预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f7325c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>predict_result</th>\n",
       "      <th>predict_score</th>\n",
       "      <th>predict_detail</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.029063967987895012</td>\n",
       "      <td>{'0': 0.970936032012105, '1': 0.02906396798789...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9970678687095642</td>\n",
       "      <td>{'0': 0.002932131290435791, '1': 0.99706786870...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.937065839767456</td>\n",
       "      <td>{'0': 0.06293416023254395, '1': 0.937065839767...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.05968008562922478</td>\n",
       "      <td>{'0': 0.9403199143707752, '1': 0.0596800856292...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9879087805747986</td>\n",
       "      <td>{'0': 0.012091219425201416, '1': 0.98790878057...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>560</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.01554611511528492</td>\n",
       "      <td>{'0': 0.9844538848847151, '1': 0.0155461151152...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>561</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7906541228294373</td>\n",
       "      <td>{'0': 0.20934587717056274, '1': 0.790654122829...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>563</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9084046483039856</td>\n",
       "      <td>{'0': 0.0915953516960144, '1': 0.9084046483039...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>565</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9931216835975647</td>\n",
       "      <td>{'0': 0.006878316402435303, '1': 0.99312168359...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>566</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9626039266586304</td>\n",
       "      <td>{'0': 0.03739607334136963, '1': 0.962603926658...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id label predict_result         predict_score  \\\n",
       "0      1   0.0              0  0.029063967987895012   \n",
       "1      4   1.0              1    0.9970678687095642   \n",
       "2      8   1.0              1     0.937065839767456   \n",
       "3      9   0.0              0   0.05968008562922478   \n",
       "4     11   1.0              1    0.9879087805747986   \n",
       "..   ...   ...            ...                   ...   \n",
       "223  560   0.0              0   0.01554611511528492   \n",
       "224  561   1.0              1    0.7906541228294373   \n",
       "225  563   1.0              1    0.9084046483039856   \n",
       "226  565   1.0              1    0.9931216835975647   \n",
       "227  566   1.0              1    0.9626039266586304   \n",
       "\n",
       "                                        predict_detail   type  \n",
       "0    {'0': 0.970936032012105, '1': 0.02906396798789...  train  \n",
       "1    {'0': 0.002932131290435791, '1': 0.99706786870...  train  \n",
       "2    {'0': 0.06293416023254395, '1': 0.937065839767...  train  \n",
       "3    {'0': 0.9403199143707752, '1': 0.0596800856292...  train  \n",
       "4    {'0': 0.012091219425201416, '1': 0.98790878057...  train  \n",
       "..                                                 ...    ...  \n",
       "223  {'0': 0.9844538848847151, '1': 0.0155461151152...  train  \n",
       "224  {'0': 0.20934587717056274, '1': 0.790654122829...  train  \n",
       "225  {'0': 0.0915953516960144, '1': 0.9084046483039...  train  \n",
       "226  {'0': 0.006878316402435303, '1': 0.99312168359...  train  \n",
       "227  {'0': 0.03739607334136963, '1': 0.962603926658...  train  \n",
       "\n",
       "[228 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_component('nn_0').get_output_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1df84b",
   "metadata": {},
   "source": [
    "## TrainerParam 训练器参数与训练器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4a6605",
   "metadata": {},
   "source": [
    "在该版本中，Homo-NN的训练逻辑，联邦聚合逻辑，都在Trainer类中实现。FATE自带一个fedavg_trainer，其中实现了标准的fedavg算法，默认情况下会在每个epoch聚合各方的模型。 而TrainerParam的作用是:\n",
    "\n",
    "- 使用trainer_name='{模块名字}'指定使用的trainer，trainer在federatedml.nn.homo.trainer目录下，因此，你可以自定义自己的trainer，自定义trainer的教程将会有专门一章\n",
    "- 其余参数将会传递到trainer的\\_\\_init\\_\\_() 接口\n",
    "\n",
    "我们可以查看下FATE自带的 fedavg_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d742b424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from federatedml.nn.homo.trainer.fedavg_trainer import FedAVGTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e9a681",
   "metadata": {},
   "source": [
    "查看FedAVGTrainer的文档，了解可用的参数，提交任务时，这些参数都可用TrainerParam传递"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "041f1937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    epochs: int >0, epochs to train\n",
      "    batch_size: int, -1 means full batch\n",
      "    early_stop: None, 'diff' or 'abs'. if None, disable early stop; if 'diff', use the loss difference between\n",
      "                two epochs as early stop condition, if differences < eps, stop training ; if 'abs', if loss < eps,\n",
      "                stop training\n",
      "    eps: float, eps value for early stop\n",
      "    secure_aggregate: bool, default is True, whether to use secure aggregation. if enabled, will add random number\n",
      "                            mask to local models. These random number masks will eventually cancel out to get 0.\n",
      "    weighted_aggregation: bool, whether add weight to each local model when doing aggregation.\n",
      "                         if True, According to origin paper, weight of a client is: n_local / n_global, where n_local\n",
      "                         is the sample number locally and n_global is the sample number of all clients.\n",
      "                         if False, simply averaging these models.\n",
      "\n",
      "    aggregate_every_n_epoch: None or int. if None, aggregate model on the end of every epoch, if int, aggregate\n",
      "                             every n epochs.\n",
      "    cuda: bool, use cuda or not\n",
      "    pin_memory: bool, for pytorch DataLoader\n",
      "    shuffle: bool, for pytorch DataLoader\n",
      "    data_loader_worker: int, for pytorch DataLoader, number of workers when loading data\n",
      "    validation_freqs: None or int. if int, validate your model and send validate results to fate-board every n epoch.\n",
      "                      if is binary classification task, will use metrics 'auc', 'ks', 'gain', 'lift', 'precision'\n",
      "                      if is multi classification task, will use metrics 'precision', 'recall', 'accuracy'\n",
      "                      if is regression task, will use metrics 'mse', 'mae', 'rmse', 'explained_variance', 'r2_score'\n",
      "    checkpoint_save_freqs: save model every n epoch, if None, will not save checkpoint.\n",
      "    task_type: str, 'auto', 'binary', 'multi', 'regression'\n",
      "               this option decides the return format of this trainer, and the evaluation type when running validation.\n",
      "               if auto, will automatically infer your task type from labels and predict results.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(FedAVGTrainer.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c519f8d",
   "metadata": {},
   "source": [
    "也可参考下接口的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ff39a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedAVGTrainer(TrainerBase):\n",
    "\n",
    "    def __init__(self, epochs=10, batch_size=512,  # training parameter\n",
    "                 early_stop=None, eps=0.0001,  # early stop parameters\n",
    "                 secure_aggregate=True, weighted_aggregation=True, aggregate_every_n_epoch=None,  # federation\n",
    "                 cuda=False, pin_memory=True, shuffle=True, data_loader_worker=0,  # GPU dataloader\n",
    "                 validation_freqs=None,  # validation configuration\n",
    "                 checkpoint_save_freqs=None,\n",
    "                 task_type='auto'\n",
    "                 ):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75609d7f",
   "metadata": {},
   "source": [
    "# Homo NN组件的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79b6724b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    name, name of this component\n",
      "    trainer, trainer param\n",
      "    dataset, dataset param\n",
      "    torch_seed, global random seed\n",
      "    loss, loss function from fate_torch\n",
      "    optimizer, optimizer from fate_torch\n",
      "    model, a fate torch sequential defining the model structure\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(HomoNN.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde0e5e8",
   "metadata": {},
   "source": [
    "至此，我们已经可以对Homo-NN有一些基本的了解，并用其实现一些基本的建模任务了，不过Homo-NN还支持对模型，数据集和训练流程的自定义，可以\n",
    "参考后面的其他文档"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
