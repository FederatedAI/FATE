{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c0046a",
   "metadata": {},
   "source": [
    "# Homo NN 自定义Trainer\n",
    "\n",
    "目前FATE自带的FedAVGTrainer仅针对常用的分类、回归任务，但是如果说有特殊的使用需求，比方说，目标检测，推荐，语义标注等，对数据集，loss和训练流程有特定的需求，则需要修改现有的训练流程。\n",
    "\n",
    "FATE-1.10除了Dataset与CustModel外，还支持对Trainer的自定义，以满足对训练流程定制化的需求: \n",
    "trainer的基类位于nn.homo.trainer.trainer_base下，\n",
    "如果需要开发自己的Trainer，你需要实现一些接口，以让FATE可以正确调用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0d6093",
   "metadata": {},
   "source": [
    "## TrainerBase接口介绍\n",
    "\n",
    "### TrainerBase的部分代码\n",
    "此处我们介绍TrainerBase的部分代码，下栏给出了部分代码，它们与你定制化自己的Trainer有关系。根据它们我们可以很快的实现一个简单的定制化Trainer\n",
    "\n",
    "- \\_\\_init\\_\\_ 你可以在这里定义Trainer需要用到的参数，如epoch, batch_size等\n",
    "\n",
    "- train接口: 你需要实现的接口，在运行时，Homo-NN component会自动调用train函数，进行训练。该接口接受四个参数，train_set, validate_set, optimizer和loss。Homo-nn component会根据你在pipeline里的设置，把你设定的训练集，验证集，optimizer和loss传到train里，请注意，optimizer会用model的parameters()实例化，而loss是pytorch loss function的实例。因此，你可以在train function里写你自己的训练流程。\n",
    "\n",
    "- self.model: 在算法运行时，在运行train前， Homo-NN component自动地调用set_model接口，设置你的模型，因此，在实现train时，你可以通过self.model来使用模型\n",
    "\n",
    "- local_mode() 和 self.fed_mode: 你可以通过local_mode() 将 self.fed_mode设置为False，在train中你可以通过fed_mode来区分本地测试的本地模式和联邦模式，这个功能你可以在你本地开发/测试的时候使用\n",
    "\n",
    "我们可以从下面一个简单的Trainer例子了解Trainer的定制化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b45b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerBase(object):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \n",
    "        self._fed_mode = True\n",
    "        self._model = None\n",
    "        ...\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        if not hasattr(self, '_model'):\n",
    "            raise AttributeError('model variable is not initialized, remember to call'\n",
    "                                 ' super(your_class, self).__init__()')\n",
    "        if self._model is None:\n",
    "            raise AttributeError('model is not set, use set_model() function to set training model')\n",
    "\n",
    "        return self._model\n",
    "\n",
    "    @model.setter\n",
    "    def model(self, val):\n",
    "        self._model = val\n",
    "        \n",
    "    def set_model(self, model: Module):\n",
    "        if not issubclass(type(model), Module):\n",
    "            raise ValueError('model must be a subclass of pytorch nn.Module')\n",
    "        self.model = model\n",
    "\n",
    "    @property\n",
    "    def fed_mode(self):\n",
    "        if not hasattr(self, '_fed_mode'):\n",
    "            raise AttributeError('run_local_mode variable is not initialized, remember to call'\n",
    "                                 ' super(your_class, self).__init__()')\n",
    "        return self._fed_mode\n",
    "\n",
    "    @fed_mode.setter\n",
    "    def fed_mode(self, val):\n",
    "        self._fed_mode = val\n",
    "\n",
    "    def local_mode(self):\n",
    "        self.fed_mode = False\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def train(self, train_set, validate_set=None, optimizer=None, loss=None):\n",
    "        \"\"\"\n",
    "            train_set : A Dataset Instance, must be a instance of subclass of Dataset (federatedml.nn.dataset.base),\n",
    "                      for example, TableDataset() (from federatedml.nn.dataset.table)\n",
    "\n",
    "            validate_set : A Dataset Instance, but optional must be a instance of subclass of Dataset\n",
    "                    (federatedml.nn.dataset.base), for example, TableDataset() (from federatedml.nn.dataset.table)\n",
    "\n",
    "            optimizer : A pytorch optimizer class instance, for example, t.optim.Adam(), t.optim.SGD()\n",
    "\n",
    "            loss : A pytorch Loss class, for example, nn.BECLoss(), nn.CrossEntropyLoss()\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7fe77b",
   "metadata": {},
   "source": [
    "## 实例1：开发一个简单的自定义Trainer\n",
    "\n",
    "这里，我们开发一个简单的自定义Trainer，以展示各个接口如何使用的：\n",
    "为了方便 这里使用save_to_fate接口保存trainer, 当然你可以直接将Trainer文件手动部署到federatedml/nn/homo/trainer下\n",
    "\n",
    "### mytrainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "459351b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.component.nn import save_to_fate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3295d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%save_to_fate trainer mytrainer.py  \n",
    "# save to federatedml/nn/homo/trainer\n",
    "import torch as t\n",
    "from federatedml.util import LOGGER\n",
    "from federatedml.nn.homo.trainer.trainer_base import TrainerBase\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 使用FATE自带的SecureAggregator，开发Trainer时，使用SeureAggregator的Client端\n",
    "from federatedml.framework.homo.aggregator.secure_aggregator import SecureAggregatorClient\n",
    "\n",
    "\n",
    "class MyTrainer(TrainerBase):\n",
    "    \n",
    "    def __init__(self, epochs, batch_size=256, dataloader_worker=4):\n",
    "        super(MyTrainer, self).__init__()\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.dataloader_worker = dataloader_worker\n",
    "        \n",
    "    # 实现train 接口\n",
    "    def train(self, train_set, val=None, optimizer=None, loss=None):\n",
    "        \n",
    "        fed_avg = None\n",
    "        LOGGER.info('run local mode is {}'.format(self.fed_mode))\n",
    "        \n",
    "        # 当调用trainer.local_mode()时，会将fed_mode设定为False，加入此判断是为了满足\n",
    "        # 本地测试的需要，可以绕过联邦的流程，SecureAggregationClient无法直接在一个本地脚本里运行\n",
    "        if self.fed_mode:\n",
    "            # max aggregate round 为多聚合轮数\n",
    "            # sample number用于计算模型权重\n",
    "            fed_avg = SecureAggregatorClient(max_aggregate_round=self.epochs, sample_number=len(train_set), secure_aggregate=True)\n",
    "            LOGGER.info('initializing fed avg')\n",
    "        \n",
    "        # dataloader + for 循环， 算的loss并backward\n",
    "        # 与pytorch的训练流程完全一致\n",
    "        dl = DataLoader(train_set, batch_size=self.batch_size, num_workers=self.dataloader_worker)\n",
    "        for epoch_idx in range(0, self.epochs):\n",
    "            l_sum = 0\n",
    "            for data, label in dl:\n",
    "                optimizer.zero_grad()\n",
    "                # self.model \n",
    "                pred = self.model(data)\n",
    "                l = loss(pred, label)\n",
    "                l.backward()\n",
    "                optimizer.step()\n",
    "                l_sum += l\n",
    "            \n",
    "            # LOGGER打印日志到log里\n",
    "            LOGGER.info('loss sum is {}'.format(l_sum))\n",
    "            \n",
    "            # 通过secure aggregator聚合模型即可\n",
    "            if fed_avg:\n",
    "                # 聚合模型与epoch loss\n",
    "                fed_avg.aggregate(self.model, l_sum.cpu().detach().numpy())\n",
    "                    \n",
    "        LOGGER.info('training finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a6f6aa",
   "metadata": {},
   "source": [
    "完成了代码 我们可以本地测试一下能否跑通\n",
    "\n",
    "## 实例1：本地测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fe8c647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "from federatedml.nn.dataset.table import TableDataset\n",
    "\n",
    "dataset = TableDataset()\n",
    "dataset.load('../examples/data/breast_homo_host.csv')\n",
    "dataset[0]\n",
    "print(dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32a9c433",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MyTrainer(epochs=10, batch_size=128, ) # 10个epoch batch_size=128\n",
    "model = t.nn.Sequential(\n",
    "    t.nn.Linear(30, 16),\n",
    "    t.nn.ReLU(),\n",
    "    t.nn.Linear(16, 1),\n",
    "    t.nn.Sigmoid()\n",
    ")\n",
    "loss = t.nn.BCELoss()  # loss function \n",
    "optimizer = t.optim.Adam(model.parameters(), lr=0.01)# optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5093ff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_model(model) # set model\n",
    "trainer.local_mode()  # local model，进行本地测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de7edd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "run local mode is False\n",
      "loss sum is 1.131636381149292\n",
      "loss sum is 0.8614962100982666\n",
      "loss sum is 0.6639648675918579\n",
      "loss sum is 0.5202457904815674\n",
      "loss sum is 0.4206005930900574\n",
      "loss sum is 0.35165560245513916\n",
      "loss sum is 0.30107438564300537\n",
      "loss sum is 0.2607121169567108\n",
      "loss sum is 0.2271752953529358\n",
      "loss sum is 0.20061872899532318\n",
      "training finished!\n"
     ]
    }
   ],
   "source": [
    "trainer.train(dataset, loss=loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b438848",
   "metadata": {},
   "source": [
    "\n",
    "## 实例1：联邦任务\n",
    "\n",
    "本地完成测试后，我们可以马上按照本地的参数提交一个fate任务了，这个任务下，有两方参与这个任务，按照mytrainer的逻辑，会每轮进行训练，然后进行聚合："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2dbd76b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'namespace': 'experiment', 'table_name': 'breast_host_1'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "from pipeline import fate_torch_hook\n",
    "from pipeline.component import HomoNN\n",
    "from pipeline.backend.pipeline import PipeLine\n",
    "from pipeline.component import Reader, Evaluation, DataTransform\n",
    "from pipeline.interface import Data, Model\n",
    "\n",
    "fate_torch_hook(t)\n",
    "\n",
    "import os\n",
    "# 绑定地址到fate name&namespace\n",
    "fate_project_path = os.path.abspath('../')\n",
    "host_0 = 10000\n",
    "host_1 = 9999\n",
    "pipeline = PipeLine().set_initiator(role='host', party_id=host_0).set_roles(host=[host_0, host_1],\n",
    "                                                                            arbiter=[host_0])\n",
    "\n",
    "data_0 = {\"name\": \"breast_host_0\", \"namespace\": \"experiment\"}\n",
    "data_1 = {\"name\": \"breast_host_1\", \"namespace\": \"experiment\"}\n",
    "\n",
    "# 为方便，本示例中两方使用同一份数据集\n",
    "data_path_0 = fate_project_path + '/examples/data/breast_homo_host.csv'\n",
    "data_path_1 = fate_project_path + '/examples/data/breast_homo_host.csv'\n",
    "pipeline.bind_table(name=data_0['name'], namespace=data_0['namespace'], path=data_path_0)\n",
    "pipeline.bind_table(name=data_1['name'], namespace=data_1['namespace'], path=data_path_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e7d5662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义reader\n",
    "reader_0 = Reader(name=\"reader_0\")\n",
    "reader_0.get_party_instance(role='host', party_id=host_0).component_param(table=data_0)\n",
    "reader_0.get_party_instance(role='host', party_id=host_1).component_param(table=data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8adf78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.component.homo_nn import TrainerParam # Trainer的接口，我们通过这个接口指定我们的trainer，并传递参数\n",
    "from pipeline.component.homo_nn import DatasetParam\n",
    "\n",
    "# 与本地测试一样的设置\n",
    "model = t.nn.Sequential(\n",
    "    t.nn.Linear(30, 16),\n",
    "    t.nn.ReLU(),\n",
    "    t.nn.Linear(16, 1),\n",
    "    t.nn.Sigmoid()\n",
    ")\n",
    "loss = t.nn.BCELoss()  # loss function \n",
    "optimizer = t.optim.Adam(model.parameters(), lr=0.01)# optimizer\n",
    "\n",
    "nn_component = HomoNN(name='nn_0',\n",
    "                      model=model, # 模型\n",
    "                      loss=loss,\n",
    "                      optimizer=optimizer,\n",
    "                      dataset=DatasetParam(dataset_name='table'),\n",
    "                      trainer=TrainerParam(trainer_name='mytrainer', epochs=10, batch_size=128)\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a7907b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pipeline.backend.pipeline.PipeLine at 0x7f6057bf0820>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 添加组件到pipeline，定义数据IO关系，提交即可\n",
    "pipeline.add_component(reader_0)\n",
    "pipeline.add_component(nn_component, data=Data(train_data=reader_0.output.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95e847a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2022-11-15 17:07:08.088\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mJob id is 202211151707072866880\n",
      "\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:08.103\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m98\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KJob is still waiting, time elapse: 0:00:00\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:09.131\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m98\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KJob is still waiting, time elapse: 0:00:01\u001b[0m\n",
      "\u001b[0mm2022-11-15 17:07:11.225\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1m\n",
      "\u001b[32m2022-11-15 17:07:11.230\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component reader_0, time elapse: 0:00:03\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:12.259\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component reader_0, time elapse: 0:00:04\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:13.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component reader_0, time elapse: 0:00:05\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:14.504\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component reader_0, time elapse: 0:00:06\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:15.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component reader_0, time elapse: 0:00:07\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:16.562\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component reader_0, time elapse: 0:00:08\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:17.591\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component reader_0, time elapse: 0:00:09\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:18.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component reader_0, time elapse: 0:00:10\u001b[0m\n",
      "\u001b[0mm2022-11-15 17:07:20.815\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1m\n",
      "\u001b[32m2022-11-15 17:07:20.817\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:12\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:21.843\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:13\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:22.871\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:14\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:23.894\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:15\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:24.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:16\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:25.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:17\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:27.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:18\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:28.054\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:19\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:29.087\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:20\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:30.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:22\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:31.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:23\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:32.377\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:24\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:33.420\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:25\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:34.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:26\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:35.504\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:27\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:36.598\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:28\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:38.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component nn_0, time elapse: 0:00:30\u001b[0m\n",
      "\u001b[0mm2022-11-15 17:07:40.368\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1m\n",
      "\u001b[32m2022-11-15 17:07:40.371\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component eval_0, time elapse: 0:00:32\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:41.431\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component eval_0, time elapse: 0:00:33\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:42.457\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component eval_0, time elapse: 0:00:34\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:43.546\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component eval_0, time elapse: 0:00:35\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:44.589\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component eval_0, time elapse: 0:00:36\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:45.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component eval_0, time elapse: 0:00:37\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:46.720\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component eval_0, time elapse: 0:00:38\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:47.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component eval_0, time elapse: 0:00:39\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:48.838\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component eval_0, time elapse: 0:00:40\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:49.871\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component eval_0, time elapse: 0:00:41\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2022-11-15 17:07:50.904\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1m\u001b[80D\u001b[1A\u001b[KRunning component eval_0, time elapse: 0:00:42\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:52.975\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1mJob is success!!! Job id is 202211151707072866880\u001b[0m\n",
      "\u001b[32m2022-11-15 17:07:52.977\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipeline.utils.invoker.job_submitter\u001b[0m:\u001b[36mmonitor_job_status\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mTotal time: 0:00:44\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pipeline.compile()\n",
    "pipeline.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0844f3ee",
   "metadata": {},
   "source": [
    "任务完成，可以在fateborad任务里看到你的日志了"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
