{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c0046a",
   "metadata": {},
   "source": [
    "# Homo NN 自定义Trainer\n",
    "\n",
    "除了Dataset与CustModel外，还支持对Trainer的自定义，以满足对训练流程的需求, traner的基类位于nn.homo.trainer.trainer_base下，\n",
    "如果需要开发自己的Trainer，你需要实现一些接口，以让FATE可以正确调用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0d6093",
   "metadata": {},
   "source": [
    "## TrainerBase接口介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b45b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cf61dd5",
   "metadata": {},
   "source": [
    "## 实例：实现一个能够聚合部分模型的Trainer - TWAFL Trainer\n",
    "\n",
    "这里参考TWAFL方法(见论文https://arxiv.org/pdf/1903.07424v1.pdf)， 我们实现一个简化版的TWAFL Trainer，它将模型分为浅层与深层，浅层的模型一般来说能够更能捕捉到数据里的有效特征（如卷积层），因此，TWAFL Trainer的特点在于，相比于一整个完整的模型，它会更频繁的聚合浅层模型，当训练达到一定轮数后，再聚合深层模型。这样的设计在联邦学习的过程中，能专注于聚合参数少的浅层，以减少参数的通信量。\n",
    "\n",
    "本实例仅为演示Trainer的定制化，代码本身未经过任何测试，请勿将其用在实际生产中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3875927e",
   "metadata": {},
   "source": [
    "### Aggregator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dff8a32",
   "metadata": {},
   "source": [
    "FATE自带SecureAggregatorClient，用于client端的模型聚合，在1.10中可以对Aggregator进行开发定制，在以后教程会提到\n",
    "\n",
    "使用SecureAggregatorClient，你需要指定最大聚合轮数n（一般为epoch)，并保证调用aggregate接口n次，\n",
    "aggregate接口接受模型参数以及该epoch的loss作为参数，返回聚合的模型，以及Loss的收敛情况（bool）\n",
    "\n",
    "代码与接口详见federatedml.framework.homo.aggregator.secure_aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62cd06b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from federatedml.framework.homo.aggregator.secure_aggregator import SecureAggregatorClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963f580c",
   "metadata": {},
   "source": [
    "### twafl_trainer.py\n",
    "下面代码给出了twafl trainer的例子，实现了trainer接口，从代码可见，本地的训练流程与编写一个pytorch的本地训练脚本区别不大，\n",
    "在实现train/predict接口时，请保证接口的参数与TrainerBase一致，如果predict\n",
    "\n",
    "**train接口不返回任何东西**\n",
    "***predict接口如果需要*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bc33cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from federatedml.util import LOGGER\n",
    "from federatedml.nn.homo.trainer.trainer_base import TrainerBase\n",
    "from torch.utils.data import DataLoader\n",
    "# 使用FATE自带的SecureAggregator，开发Trainer时，只需要使用SeureAggregator的Client端\n",
    "from federatedml.framework.homo.aggregator.secure_aggregator import SecureAggregatorClient\n",
    "\n",
    "\n",
    "class TWAFLTrainer(TrainerBase):\n",
    "    \n",
    "    def __init__(self, epochs, batch_size=256, dataloader_worker=4, deep_agg_round=10):\n",
    "        super(CustTrainer, self).__init__()\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.dataloader_worker = dataloader_worker\n",
    "        self.deep_agg_round = deep_agg_round\n",
    "        \n",
    "    # 实现train 接口\n",
    "    def train(self, train_set, val=None, optimizer=None, loss=None):\n",
    "        \n",
    "        fed_avg = None\n",
    "        LOGGER.info('run local mode is {}'.format(self.fed_mode))\n",
    "        \n",
    "        # 当调用trainer.local_mode()时，会将fed_mode设定为False，加入此判断是为了满足\n",
    "        # 本地测试的需要，可以绕过联邦的流程，否则会报错\n",
    "        if self.fed_mode:\n",
    "            # max aggregate round 为多聚合轮数\n",
    "            # sample number用于计算模型权重\n",
    "            fed_avg = SecureAggregatorClient(max_aggregate_round=self.epochs, sample_number=len(train_set), secure_aggregate=True)\n",
    "            LOGGER.info('initializing fed avg')\n",
    "        \n",
    "        # dataloader + for 循环， 算的loss并backward\n",
    "        # 与pytorch的训练流程完全一致\n",
    "        dl = DataLoader(train_set, batch_size=self.batch_size, num_workers=self.dataloader_worker)\n",
    "        for epoch_idx in range(0, self.epochs):\n",
    "            l_sum = 0\n",
    "            for data, label in dl:\n",
    "                optimizer.zero_grad()\n",
    "                pred = self.model(data)\n",
    "                l = loss(pred, label)\n",
    "                l.backward()\n",
    "                optimizer.step()\n",
    "                l_sum += l\n",
    "                \n",
    "            LOGGER.info('loss sum is {}'.format(l_sum))\n",
    "            \n",
    "            # 通过secure aggregator聚合模型即可\n",
    "            if fed_avg:\n",
    "                if (epoch_idx + 1) % self.deep_agg_round == 0:  # 当满足一定轮数，我们聚合完整模型与loss\n",
    "                    fed_avg.aggregate(self.model, l_sum.cpu().detach().numpy())\n",
    "                else:\n",
    "                    # 否则 仅仅聚合浅层模型 这要求模型提供属性shallow_model\n",
    "                    fed_avg.aggregate(self.model.shallow_model, l_sum.cpu().detach().numpy()) \n",
    "                    \n",
    "        LOGGER.info('training finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7d5662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
