{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c0046a",
   "metadata": {},
   "source": [
    "# Homo NN 自定义Trainer\n",
    "\n",
    "目前FATE自带的FedAVGTrainer仅针对常用的分类、回归任务，但是如果说有特殊的使用需求，比方说，目标检测，推荐，语义标注等，对数据集，loss和训练流程有特定的需求，则需要修改现有的训练流程。\n",
    "\n",
    "FATE-1.10除了Dataset与CustModel外，还支持对Trainer的自定义，以满足对训练流程定制化的需求: \n",
    "trainer的基类位于nn.homo.trainer.trainer_base下，\n",
    "如果需要开发自己的Trainer，你需要实现一些接口，以让FATE可以正确调用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0d6093",
   "metadata": {},
   "source": [
    "## TrainerBase接口介绍\n",
    "\n",
    "### TrainerBase的部分代码\n",
    "此处我们介绍TrainerBase的部分代码，下栏给出了部分代码，它们与你定制化自己的Trainer有关系。根据它们我们可以很快的实现一个简单的定制化Trainer\n",
    "\n",
    "- \\_\\_init\\_\\_ 你可以在这里定义Trainer需要用到的参数，如epoch, batch_size等\n",
    "\n",
    "- train接口: 你需要实现的接口，在运行时，Homo-NN component会自动调用train函数，进行训练。该接口接受四个参数，train_set, validate_set, optimizer和loss。Homo-nn component会根据你在pipeline里的设置，把你设定的训练集，验证集，optimizer和loss传到train里，请注意，optimizer是实例化的torch优化器，而loss是实例化的pytorch loss function。因此，你可以在train function里写你自己的训练流程，想怎么写都可以\n",
    "\n",
    "- model属性: 在算法运行时，在运行train前， Homo-NN component自动地调用set_model接口，设置你的模型，因此，在实现train时，你可以通过self.model来使用模型\n",
    "\n",
    "- local_mode() 和 self.fed_mode: 你可以通过local_mode() 将 self.fed_mode设置为False，在train中你可以通过fed_mode来区分本地测试的本地模式和联邦模式，这个功能你可以在你本地开发/测试的时候使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b45b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerBase(object):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \n",
    "        self._fed_mode = True\n",
    "        self._model = None\n",
    "        ...\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        if not hasattr(self, '_model'):\n",
    "            raise AttributeError('model variable is not initialized, remember to call'\n",
    "                                 ' super(your_class, self).__init__()')\n",
    "        if self._model is None:\n",
    "            raise AttributeError('model is not set, use set_model() function to set training model')\n",
    "\n",
    "        return self._model\n",
    "\n",
    "    @model.setter\n",
    "    def model(self, val):\n",
    "        self._model = val\n",
    "        \n",
    "    def set_model(self, model: Module):\n",
    "        if not issubclass(type(model), Module):\n",
    "            raise ValueError('model must be a subclass of pytorch nn.Module')\n",
    "        self.model = model\n",
    "\n",
    "    @property\n",
    "    def fed_mode(self):\n",
    "        if not hasattr(self, '_fed_mode'):\n",
    "            raise AttributeError('run_local_mode variable is not initialized, remember to call'\n",
    "                                 ' super(your_class, self).__init__()')\n",
    "        return self._fed_mode\n",
    "\n",
    "    @fed_mode.setter\n",
    "    def fed_mode(self, val):\n",
    "        self._fed_mode = val\n",
    "\n",
    "    def local_mode(self):\n",
    "        self.fed_mode = False\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def train(self, train_set, validate_set=None, optimizer=None, loss=None):\n",
    "        \"\"\"\n",
    "            train_set : A Dataset Instance, must be a instance of subclass of Dataset (federatedml.nn.dataset.base),\n",
    "                      for example, TableDataset() (from federatedml.nn.dataset.table)\n",
    "\n",
    "            validate_set : A Dataset Instance, but optional must be a instance of subclass of Dataset\n",
    "                    (federatedml.nn.dataset.base), for example, TableDataset() (from federatedml.nn.dataset.table)\n",
    "\n",
    "            optimizer : A pytorch optimizer class instance, for example, t.optim.Adam(), t.optim.SGD()\n",
    "\n",
    "            loss : A pytorch Loss class, for example, nn.BECLoss(), nn.CrossEntropyLoss()\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7fe77b",
   "metadata": {},
   "source": [
    "## 实例1：开发一个简单的自定义Trainer\n",
    "\n",
    "这里，我们开发一个简单的自定义Trainer，以展示各个接口如何使用的：\n",
    "\n",
    "### mytrainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3295d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from federatedml.util import LOGGER\n",
    "from federatedml.nn.homo.trainer.trainer_base import TrainerBase\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 使用FATE自带的SecureAggregator，开发Trainer时，使用SeureAggregator的Client端\n",
    "from federatedml.framework.homo.aggregator.secure_aggregator import SecureAggregatorClient\n",
    "\n",
    "\n",
    "class MyTrainer(TrainerBase):\n",
    "    \n",
    "    def __init__(self, epochs, batch_size=256, dataloader_worker=4):\n",
    "        super(MyTrainer, self).__init__()\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.dataloader_worker = dataloader_worker\n",
    "        \n",
    "    # 实现train 接口\n",
    "    def train(self, train_set, val=None, optimizer=None, loss=None):\n",
    "        \n",
    "        fed_avg = None\n",
    "        LOGGER.info('run local mode is {}'.format(self.fed_mode))\n",
    "        \n",
    "        # 当调用trainer.local_mode()时，会将fed_mode设定为False，加入此判断是为了满足\n",
    "        # 本地测试的需要，可以绕过联邦的流程，否则会报错\n",
    "        if self.fed_mode:\n",
    "            # max aggregate round 为多聚合轮数\n",
    "            # sample number用于计算模型权重\n",
    "            fed_avg = SecureAggregatorClient(max_aggregate_round=self.epochs, sample_number=len(train_set), secure_aggregate=True)\n",
    "            LOGGER.info('initializing fed avg')\n",
    "        \n",
    "        # dataloader + for 循环， 算的loss并backward\n",
    "        # 与pytorch的训练流程完全一致\n",
    "        dl = DataLoader(train_set, batch_size=self.batch_size, num_workers=self.dataloader_worker)\n",
    "        for epoch_idx in range(0, self.epochs):\n",
    "            l_sum = 0\n",
    "            for data, label in dl:\n",
    "                optimizer.zero_grad()\n",
    "                # self.model \n",
    "                pred = self.model(data)\n",
    "                l = loss(pred, label)\n",
    "                l.backward()\n",
    "                optimizer.step()\n",
    "                l_sum += l\n",
    "            \n",
    "            # LOGGER打印日志到log里\n",
    "            LOGGER.info('loss sum is {}'.format(l_sum))\n",
    "            \n",
    "            # 通过secure aggregator聚合模型即可\n",
    "            if fed_avg:\n",
    "                # 聚合模型与epoch loss\n",
    "                fed_avg.aggregate(self.model, l_sum.cpu().detach().numpy())\n",
    "                    \n",
    "        LOGGER.info('training finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a6f6aa",
   "metadata": {},
   "source": [
    "## 实例1：本地测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fe8c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from federatedml.nn.dataset.\n",
    "\n",
    "trainer = MyTrainer(epochs=10, batch_size=128, )\n",
    "model = t.nn.Sequential(\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf61dd5",
   "metadata": {},
   "source": [
    "## 实例：实现一个能够聚合部分模型的Trainer - TWAFL Trainer\n",
    "\n",
    "这里参考TWAFL方法(见论文https://arxiv.org/pdf/1903.07424v1.pdf)， 我们实现一个简化版的TWAFL Trainer，它将模型分为浅层与深层，浅层的模型一般来说能够更能捕捉到数据里的有效特征（如卷积层），因此，TWAFL Trainer的特点在于，相比于一整个完整的模型，它会更频繁的聚合浅层模型，当训练达到一定轮数后，再聚合深层模型。这样的设计在联邦学习的过程中，能专注于聚合参数少的浅层，以减少参数的通信量。\n",
    "\n",
    "本实例仅为演示Trainer的定制化，代码本身未经过任何测试，请勿将其用在实际生产中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3875927e",
   "metadata": {},
   "source": [
    "### Aggregator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dff8a32",
   "metadata": {},
   "source": [
    "FATE自带SecureAggregatorClient，用于client端的模型聚合，在1.10中可以对Aggregator进行开发定制，在以后教程会提到\n",
    "\n",
    "使用SecureAggregatorClient，你需要指定最大聚合轮数n（一般为epoch)，并保证调用aggregate接口n次，\n",
    "aggregate接口接受模型参数以及该epoch的loss作为参数，返回聚合的模型，以及Loss的收敛情况（bool）\n",
    "\n",
    "代码与接口详见federatedml.framework.homo.aggregator.secure_aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62cd06b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from federatedml.framework.homo.aggregator.secure_aggregator import SecureAggregatorClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963f580c",
   "metadata": {},
   "source": [
    "### twafl_trainer.py\n",
    "下面代码给出了twafl trainer的例子，实现了trainer接口，从代码可见，本地的训练流程与编写一个pytorch的本地训练脚本区别不大，\n",
    "在实现train/predict接口时，请保证接口的参数与TrainerBase一致，如果predict\n",
    "\n",
    "**train接口不返回任何东西**\n",
    "***predict接口如果需要*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bc33cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from federatedml.util import LOGGER\n",
    "from federatedml.nn.homo.trainer.trainer_base import TrainerBase\n",
    "from torch.utils.data import DataLoader\n",
    "# 使用FATE自带的SecureAggregator，开发Trainer时，只需要使用SeureAggregator的Client端\n",
    "from federatedml.framework.homo.aggregator.secure_aggregator import SecureAggregatorClient\n",
    "\n",
    "\n",
    "class TWAFLTrainer(TrainerBase):\n",
    "    \n",
    "    def __init__(self, epochs, batch_size=256, dataloader_worker=4, deep_agg_round=10):\n",
    "        super(CustTrainer, self).__init__()\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.dataloader_worker = dataloader_worker\n",
    "        self.deep_agg_round = deep_agg_round\n",
    "        \n",
    "    # 实现train 接口\n",
    "    def train(self, train_set, val=None, optimizer=None, loss=None):\n",
    "        \n",
    "        fed_avg = None\n",
    "        LOGGER.info('run local mode is {}'.format(self.fed_mode))\n",
    "        \n",
    "        # 当调用trainer.local_mode()时，会将fed_mode设定为False，加入此判断是为了满足\n",
    "        # 本地测试的需要，可以绕过联邦的流程，否则会报错\n",
    "        if self.fed_mode:\n",
    "            # max aggregate round 为多聚合轮数\n",
    "            # sample number用于计算模型权重\n",
    "            fed_avg = SecureAggregatorClient(max_aggregate_round=self.epochs, sample_number=len(train_set), secure_aggregate=True)\n",
    "            LOGGER.info('initializing fed avg')\n",
    "        \n",
    "        # dataloader + for 循环， 算的loss并backward\n",
    "        # 与pytorch的训练流程完全一致\n",
    "        dl = DataLoader(train_set, batch_size=self.batch_size, num_workers=self.dataloader_worker)\n",
    "        for epoch_idx in range(0, self.epochs):\n",
    "            l_sum = 0\n",
    "            for data, label in dl:\n",
    "                optimizer.zero_grad()\n",
    "                pred = self.model(data)\n",
    "                l = loss(pred, label)\n",
    "                l.backward()\n",
    "                optimizer.step()\n",
    "                l_sum += l\n",
    "                \n",
    "            LOGGER.info('loss sum is {}'.format(l_sum))\n",
    "            \n",
    "            # 通过secure aggregator聚合模型即可\n",
    "            if fed_avg:\n",
    "                if (epoch_idx + 1) % self.deep_agg_round == 0:  # 当满足一定轮数，我们聚合完整模型与loss\n",
    "                    fed_avg.aggregate(self.model, l_sum.cpu().detach().numpy())\n",
    "                else:\n",
    "                    # 否则 仅仅聚合浅层模型 这要求模型提供属性shallow_model\n",
    "                    fed_avg.aggregate(self.model.shallow_model, l_sum.cpu().detach().numpy()) \n",
    "                    \n",
    "        LOGGER.info('training finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7d5662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
